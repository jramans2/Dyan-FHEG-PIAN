{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GATConv\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "class DHGANN(nn.Module):\n",
        "    \"\"\"\n",
        "    Dynamic Heterogeneous Graph Attention Network (DHGANN).\n",
        "\n",
        "    This model represents financial features as nodes in a graph and learns the\n",
        "    dynamic relationships between them using a Graph Attention Network (GAT).\n",
        "\n",
        "    It's \"heterogeneous\" in the sense that nodes (features) are of different types\n",
        "    (e.g., price, volume, momentum), and \"dynamic\" because the graph structure\n",
        "    and attention weights can change with each input.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_heads=4):\n",
        "        super(DHGANN, self).__init__()\n",
        "        self.input_dim = input_dim # This will be the sequence length\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        # We treat each feature type as a node. The input to the GAT will be a feature matrix\n",
        "        # where each row is a node embedding. We use a simple linear layer to create initial embeddings.\n",
        "        self.node_embedding = nn.Linear(self.input_dim, hidden_dim)\n",
        "\n",
        "        # Graph Attention Layers\n",
        "        self.gat_conv1 = GATConv(hidden_dim, hidden_dim, heads=num_heads, concat=True)\n",
        "        self.gat_conv2 = GATConv(hidden_dim * num_heads, output_dim, heads=1, concat=False)\n",
        "\n",
        "        self.elu = nn.ELU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass for DHGANN.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): The input time-series data of shape\n",
        "                              (batch_size, sequence_length, num_features).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: A graph-based feature representation of shape (batch_size, output_dim).\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, num_features = x.shape\n",
        "\n",
        "        # We want to model relationships between features. So we transpose the input\n",
        "        # to treat features as nodes and the time sequence as node features.\n",
        "        # Shape becomes (batch_size, num_features, seq_len)\n",
        "        x = x.permute(0, 2, 1)\n",
        "\n",
        "        # Process each item in the batch\n",
        "        batch_outputs = []\n",
        "        for i in range(batch_size):\n",
        "            # Get the node features for the current graph in the batch\n",
        "            # Shape: (num_features, seq_len)\n",
        "            node_features = x[i]\n",
        "\n",
        "            # 1. Create initial node embeddings\n",
        "            # Shape: (num_features, hidden_dim)\n",
        "            h = self.node_embedding(node_features)\n",
        "\n",
        "            # 2. Dynamically create the graph structure (edge_index)\n",
        "            # For simplicity, we create a fully connected graph where every feature\n",
        "            # is connected to every other feature. A more complex model could learn\n",
        "            # the edge structure.\n",
        "            num_nodes = num_features\n",
        "            edge_index = self._create_fully_connected_graph(num_nodes, h.device)\n",
        "\n",
        "            # 3. Apply Graph Attention Layers\n",
        "            h = self.elu(self.gat_conv1(h, edge_index))\n",
        "            h = self.gat_conv2(h, edge_index)\n",
        "\n",
        "            # 4. Aggregate node features to get a single graph representation\n",
        "            # We use mean pooling over all nodes.\n",
        "            graph_embedding = torch.mean(h, dim=0)\n",
        "            batch_outputs.append(graph_embedding)\n",
        "\n",
        "        # Stack the outputs for the batch\n",
        "        output = torch.stack(batch_outputs, dim=0)\n",
        "        return output\n",
        "\n",
        "    def _create_fully_connected_graph(self, num_nodes, device):\n",
        "        \"\"\"Helper to create a fully connected edge index.\"\"\"\n",
        "        nodes = torch.arange(num_nodes, device=device)\n",
        "        # Create all possible pairs of nodes (edges)\n",
        "        edge_index = torch.cartesian_prod(nodes, nodes).t()\n",
        "        # Remove self-loops\n",
        "        edge_index = edge_index[:, edge_index[0] != edge_index[1]]\n",
        "        return edge_index\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Example Usage\n",
        "    BATCH_SIZE = 4\n",
        "    SEQ_LEN = 60\n",
        "    NUM_FEATURES = 10\n",
        "    HIDDEN_DIM = 128\n",
        "    OUTPUT_DIM = 64 # Dimension of the final graph embedding\n",
        "\n",
        "    model = DHGANN(\n",
        "        input_dim=SEQ_LEN,\n",
        "        hidden_dim=HIDDEN_DIM,\n",
        "        output_dim=OUTPUT_DIM\n",
        "    )\n",
        "\n",
        "    # Dummy input tensor (batch, seq_len, num_features)\n",
        "    dummy_input = torch.randn(BATCH_SIZE, SEQ_LEN, NUM_FEATURES)\n",
        "\n",
        "    # Get the output\n",
        "    output = model(dummy_input)\n",
        "\n",
        "    print(\"--- DHGANN Example ---\")\n",
        "    print(f\"Input shape: {dummy_input.shape}\")\n",
        "    print(f\"Output graph embedding shape: {output.shape}\") # Should be (BATCH_SIZE, OUTPUT_DIM)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "RewGWlSu5qlR"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}