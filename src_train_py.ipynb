{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from src.data_loader import get_data\n",
        "from src.model import DyanFHEGPIAN\n",
        "from src.poa import PufferfishOptimizer\n",
        "from src.utils import create_results_dir, plot_training_history, calculate_metrics\n",
        "\n",
        "# --- Configuration ---\n",
        "TICKER = \"MSFT\"\n",
        "SEQ_LEN = 60 # Use 60 days of data to predict the next day\n",
        "TARGET_COL = 'Close'\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "def create_sequences(data, seq_length):\n",
        "    \"\"\"Creates sequences and corresponding labels.\"\"\"\n",
        "    xs, ys = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        x = data[i:(i + seq_length)]\n",
        "        y = data[i + seq_length]\n",
        "        xs.append(x)\n",
        "        ys.append(y)\n",
        "    return np.array(xs), np.array(ys)\n",
        "\n",
        "def prepare_data():\n",
        "    \"\"\"Fetches, processes, and prepares data for the model.\"\"\"\n",
        "    df = get_data(ticker=TICKER)\n",
        "\n",
        "    # Select features and target\n",
        "    features = df.drop(columns=[TARGET_COL])\n",
        "    target = df[[TARGET_COL]]\n",
        "\n",
        "    # Scale the data\n",
        "    scaler_features = MinMaxScaler()\n",
        "    scaler_target = MinMaxScaler()\n",
        "\n",
        "    features_scaled = scaler_features.fit_transform(features)\n",
        "    target_scaled = scaler_target.fit_transform(target)\n",
        "\n",
        "    # Combine back for sequencing\n",
        "    data_scaled = np.concatenate([features_scaled, target_scaled], axis=1)\n",
        "\n",
        "    # Create sequences\n",
        "    X, y = create_sequences(data_scaled, SEQ_LEN)\n",
        "\n",
        "    # The target in y is the last column of the sequence\n",
        "    y = y[:, -1]\n",
        "\n",
        "    # Split data\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
        "\n",
        "    # Convert to tensors\n",
        "    X_train = torch.tensor(X_train, dtype=torch.float32).to(DEVICE)\n",
        "    y_train = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1).to(DEVICE)\n",
        "    X_val = torch.tensor(X_val, dtype=torch.float32).to(DEVICE)\n",
        "    y_val = torch.tensor(y_val, dtype=torch.float32).unsqueeze(1).to(DEVICE)\n",
        "\n",
        "    print(f\"Training data shape: {X_train.shape}\")\n",
        "    print(f\"Validation data shape: {X_val.shape}\")\n",
        "\n",
        "    return X_train, y_train, X_val, y_val, scaler_target\n",
        "\n",
        "def objective_function(params):\n",
        "    \"\"\"\n",
        "    The objective function for the Pufferfish Optimizer.\n",
        "    It trains the model with a given set of hyperparameters and returns the validation loss.\n",
        "    \"\"\"\n",
        "    # Unpack hyperparameters\n",
        "    lr = params['lr']\n",
        "    embed_dim = params['embed_dim']\n",
        "    hidden_dim = params['hidden_dim']\n",
        "    num_heads = params['num_heads']\n",
        "    lambda_physics = params['lambda_physics']\n",
        "    epochs = 30 # Use a fixed number of epochs for each POA evaluation\n",
        "\n",
        "    # --- Model Setup ---\n",
        "    input_dim = X_train.shape[2]\n",
        "\n",
        "    maxvit_params = {'embed_dim': embed_dim, 'num_heads': num_heads, 'num_blocks': 2}\n",
        "    feinn_params = {'hidden_dim': hidden_dim, 'output_dim': 1, 'n_layers': 2}\n",
        "    dhgann_params = {'hidden_dim': hidden_dim, 'output_dim': hidden_dim // 2, 'num_heads': num_heads}\n",
        "\n",
        "    model = DyanFHEGPIAN(\n",
        "        input_dim=input_dim,\n",
        "        seq_len=SEQ_LEN,\n",
        "        maxvit_params=maxvit_params,\n",
        "        feinn_params=feinn_params,\n",
        "        dhgann_params=dhgann_params\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=32, shuffle=True)\n",
        "\n",
        "    # --- Training Loop ---\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for x_batch, y_batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Main prediction\n",
        "            y_pred, features_for_loss = model(x_batch)\n",
        "            prediction_loss = criterion(y_pred, y_batch)\n",
        "\n",
        "            # Physics-informed loss from FEINN\n",
        "            feinn_module = model.feinn\n",
        "            feinn_displacement = feinn_module(features_for_loss)\n",
        "            physics_loss = feinn_module.compute_physics_loss(features_for_loss, feinn_displacement)\n",
        "\n",
        "            # Total loss\n",
        "            total_loss = prediction_loss + lambda_physics * physics_loss\n",
        "\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # --- Evaluation ---\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        y_val_pred, _ = model(X_val)\n",
        "        val_loss = criterion(y_val_pred, y_val).item()\n",
        "\n",
        "    print(f\"Params: {params} -> Validation Loss: {val_loss:.6f}\")\n",
        "    return val_loss\n",
        "\n",
        "def train_final_model(params):\n",
        "    \"\"\"\n",
        "    Trains the final model using the best hyperparameters found by POA.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Training Final Model with Best Hyperparameters ---\")\n",
        "\n",
        "    # Unpack best hyperparameters\n",
        "    lr = params['lr']\n",
        "    embed_dim = params['embed_dim']\n",
        "    hidden_dim = params['hidden_dim']\n",
        "    num_heads = params['num_heads']\n",
        "    lambda_physics = params['lambda_physics']\n",
        "    epochs = 100 # Train for more epochs on the final model\n",
        "\n",
        "    # Model Setup\n",
        "    input_dim = X_train.shape[2]\n",
        "    maxvit_params = {'embed_dim': embed_dim, 'num_heads': num_heads, 'num_blocks': 2}\n",
        "    feinn_params = {'hidden_dim': hidden_dim, 'output_dim': 1, 'n_layers': 2}\n",
        "    dhgann_params = {'hidden_dim': hidden_dim, 'output_dim': hidden_dim // 2, 'num_heads': num_heads}\n",
        "\n",
        "    model = DyanFHEGPIAN(\n",
        "        input_dim=input_dim,\n",
        "        seq_len=SEQ_LEN,\n",
        "        maxvit_params=maxvit_params,\n",
        "        feinn_params=feinn_params,\n",
        "        dhgann_params=dhgann_params\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=32, shuffle=True)\n",
        "    val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=32)\n",
        "\n",
        "    history = {'loss': [], 'val_loss': [], 'accuracy': [], 'val_accuracy': []}\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        for x_batch, y_batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            y_pred, features = model(x_batch)\n",
        "            pred_loss = criterion(y_pred, y_batch)\n",
        "            physics_loss = model.feinn.compute_physics_loss(features, model.feinn(features))\n",
        "            total_loss = pred_loss + lambda_physics * physics_loss\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += total_loss.item()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_epoch_loss = 0\n",
        "        y_true_cls, y_pred_cls = [], []\n",
        "        with torch.no_grad():\n",
        "            for x_batch, y_batch in val_loader:\n",
        "                y_val_pred, _ = model(x_batch)\n",
        "                val_epoch_loss += criterion(y_val_pred, y_batch).item()\n",
        "\n",
        "                # For accuracy metric\n",
        "                y_true_cls.extend((y_batch[1:] > y_batch[:-1]).cpu().numpy())\n",
        "                y_pred_cls.extend((y_val_pred[1:] > y_val_pred[:-1]).cpu().numpy())\n",
        "\n",
        "        avg_loss = epoch_loss / len(train_loader)\n",
        "        avg_val_loss = val_epoch_loss / len(val_loader)\n",
        "\n",
        "        # Calculate directional accuracy\n",
        "        acc = calculate_metrics(np.array(y_true_cls), np.array(y_pred_cls))['Accuracy']\n",
        "\n",
        "        history['loss'].append(avg_loss)\n",
        "        history['val_loss'].append(avg_val_loss)\n",
        "        history['accuracy'].append(acc) # Simplified accuracy for history\n",
        "        history['val_accuracy'].append(acc)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs} -> Loss: {avg_loss:.6f}, Val Loss: {avg_val_loss:.6f}, Val Acc: {acc:.4f}\")\n",
        "\n",
        "    # Save the model\n",
        "    torch.save(model.state_dict(), 'dyan_fheg_pian_model.pth')\n",
        "    print(\"Final model saved to dyan_fheg_pian_model.pth\")\n",
        "\n",
        "    return model, history\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    create_results_dir()\n",
        "    X_train, y_train, X_val, y_val, scaler_target = prepare_data()\n",
        "\n",
        "    # --- POA Optimization ---\n",
        "    param_bounds = {\n",
        "        'lr': (1e-4, 1e-2, 'float'),\n",
        "        'embed_dim': (32, 128, 'int'),\n",
        "        'hidden_dim': (64, 256, 'int'),\n",
        "        'num_heads': (2, 8, 'int'),\n",
        "        'lambda_physics': (0.01, 0.5, 'float')\n",
        "    }\n",
        "\n",
        "    # NOTE: POA is computationally expensive. For a quick run, reduce population and generations.\n",
        "    poa = PufferfishOptimizer(\n",
        "        objective_function=objective_function,\n",
        "        param_bounds=param_bounds,\n",
        "        population_size=5, # Reduced for speed\n",
        "        max_generations=3  # Reduced for speed\n",
        "    )\n",
        "\n",
        "    best_params, _ = poa.optimize()\n",
        "\n",
        "    # --- Final Training ---\n",
        "    final_model, history = train_final_model(best_params)\n",
        "\n",
        "    # --- Final Evaluation ---\n",
        "    plot_training_history(history)\n",
        "\n",
        "    final_model.eval()\n",
        "    with torch.no_grad():\n",
        "        final_preds_scaled = final_model(X_val)[0]\n",
        "\n",
        "    # Inverse transform predictions\n",
        "    final_preds = scaler_target.inverse_transform(final_preds_scaled.cpu().numpy())\n",
        "    y_val_actual = scaler_target.inverse_transform(y_val.cpu().numpy())\n",
        "\n",
        "    calculate_metrics(y_val_actual, final_preds)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "0YwA0ppN6T3r"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}